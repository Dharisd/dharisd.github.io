<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Finetuning VITS on Common voice Dhivehi - dharisd's blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Text to speech has been something that interested and involved me for sometime, And the last publically available models are from 2021 era, Since then there has been lots of improvements in the area .
I decided to cook up some and see how it goes
The model we are gonna be training the is the VITS implementaion from coqui (check them out they are the coolest fr)
Whats VITS and why VITS out of all VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech) is a autoregressive end to end model, while most other architectures such as tacotron rely on a two-stage method where a mel spectogram is first generated, which is then converted to speech using a vocoder , VITS uses a single model architecture which takes in text and returns the raw audio waveforms."><meta property="og:image" content><meta property="og:title" content="Finetuning VITS on Common voice Dhivehi"><meta property="og:description" content="Text to speech has been something that interested and involved me for sometime, And the last publically available models are from 2021 era, Since then there has been lots of improvements in the area .
I decided to cook up some and see how it goes
The model we are gonna be training the is the VITS implementaion from coqui (check them out they are the coolest fr)
Whats VITS and why VITS out of all VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech) is a autoregressive end to end model, while most other architectures such as tacotron rely on a two-stage method where a mel spectogram is first generated, which is then converted to speech using a vocoder , VITS uses a single model architecture which takes in text and returns the raw audio waveforms."><meta property="og:type" content="article"><meta property="og:url" content="https://dharisd.github.io/posts/finetuning-vits-dv/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-07T08:22:22+05:00"><meta property="article:modified_time" content="2023-02-07T08:22:22+05:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Finetuning VITS on Common voice Dhivehi"><meta name=twitter:description content="Text to speech has been something that interested and involved me for sometime, And the last publically available models are from 2021 era, Since then there has been lots of improvements in the area .
I decided to cook up some and see how it goes
The model we are gonna be training the is the VITS implementaion from coqui (check them out they are the coolest fr)
Whats VITS and why VITS out of all VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech) is a autoregressive end to end model, while most other architectures such as tacotron rely on a two-stage method where a mel spectogram is first generated, which is then converted to speech using a vocoder , VITS uses a single model architecture which takes in text and returns the raw audio waveforms."><script src=https://dharisd.github.io/js/feather.min.js></script>
<link href=https://dharisd.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://dharisd.github.io/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://dharisd.github.io/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css></head><body><div class=content><header><div class=main><a href=https://dharisd.github.io/>dharisd's blog</a></div><nav><a href=/posts>Home</a>
<a href=/posts>posts</a>
<a href=/about>About</a></nav></header><main><article><div class=title><h1 class=title>Finetuning VITS on Common voice Dhivehi</h1><div class=meta>Posted on Feb 7, 2023</div></div><section class=body><p>Text to speech has been something that interested and involved me for sometime, And the last publically available models are from 2021 era, Since then there has been lots of improvements in the area .</p><p>I decided to cook up some and see how it goes</p><p>The model we are gonna be training the is the VITS implementaion from <a href=https://coqui.ai/>coqui</a> (check them out they are the coolest fr)</p><h2 id=whats-vits-and-why-vits-out-of-all>Whats VITS and why VITS out of all</h2><p>VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech) is a autoregressive end to end model, while most other architectures such as tacotron rely on a two-stage method where a mel spectogram is first generated, which is then converted to speech using a vocoder , VITS uses a single model architecture which takes in text and returns the raw audio waveforms.</p><p><strong>Single model architecture</strong>
The model does not need to generate immediate representations and then load them again to another model for inference, this geatly reduces full end to end inference speed, this also helps to make deploynet easier</p><p><strong>Multispeaker Support</strong>
Supports training and infernce of audio for multi speakers from a single model throigh speaker embeddings.</p><p><strong>Tooling</strong>
<a href=https://github.com/coqui-ai/TTS>Coqui/TTS</a> have done an absolutely amazing job, developing the tooling and the tools to train TTS models</p><h3 id=setting-up-the-dataset>Setting up the dataset</h3><p>Setting up the dataset would have been pretty straight forward since i was using the common voice dataset, all I needed to do was to remove the voices and entries of the voices i didnt want to train on</p><p><strong>Why remove some voices?</strong></p><p>The Common Voice dataset isnt and wasnt meant to be used for Text to speech purposes as it is more or less focused on Speech to text applications, as such the voices necessasrily dont focus on sounding like TTS and most speakers sound like they are talking casually to their parents or reading out something</p><p><strong>What do?</strong></p><p>Although most sound like this I noticed that some speakers infact actually have recorded with good setups and put care into sounding the best they can</p><p>I isolated the best sounding speakers with the most audio (5 of them) each with atleast 3 hoiurs of audio to seperate dataset and then used this for training, This dataset was about x hours long</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#the client_id for the 5 speakers which sounded the best to me and had enough data</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>speakers <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;MCV_d0c4ff6121bfdbbf162900de8e68c2f5ea1e1d08391e4928e9f2febf82869a5aae10cb3e9d3f6b77487aad40da413e27ebf451dd980c01937dd7476c01df330a&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;MCV_e3de19abdf1e78628e9d0b9eceb1022db3270200e46cc30440f7318eef8c93e8fa2f52baab585edbdbb55bd14edcf67dab85d0af8248d3b0a58ea68ffbf421d8&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;MCV_0a15c5b1210dabdccec22077ac63c2b2833c170baf61e7b81a7aad821a97f4415afbd00ae54aa633c7f691eed09683948f27e7ffe6416411b3e4b6ceffaff16d&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;MCV_52bc327bd5bde02bfe73b0321673eaf6734ad583270c0d316a9409b779f3ee3817c55744164fe9925c70e29ce3fd6c2f322f856cb256957e27eb8345c9dcb62b&#34;</span>,
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;MCV_f5a79d21b4ee20e7008f784477fae663361e7cb5d1f06ea88a7d246502364c6c1b1d273db9334ba1feb1ee712e349c9a50425c52345c95a3d2d2b5c9d59ee6b4&#34;</span>
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p><strong>Dataset done right?</strong>
Just set Common voice as formatter and we move right? right?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset_config <span style=color:#f92672>=</span> BaseDatasetConfig(
</span></span><span style=display:flex><span>formatter<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;common_voice&#34;</span>, meta_file_train<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;phonemise_fixed.tsv&#34;</span>, path<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(output_path, <span style=color:#e6db74>&#34;dv_filtered/&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p><strong>NO</strong>
This just hanged infinetly when i tried setting up the Trainer
<strong>but why?</strong>
Turns out the formatter wasnt updated for the latest dataset format and was hanging due to an infinite loop</p><p>after I fixed this bug , dataset was done</p><h2 id=lets-train-lets-go-bro>LETS TRAIN LETS GO BRO</h2><p>Initially I trained from scratch but the performance at 20k steps was literally this</p><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/initial.wav type=audio/mpeg></audio><figcaption>intially sounded like this</figcaption></figure><p>so I decided to pull my favoruite trick, finetune on top of an english model</p><p><strong>just pull out the model</strong>
<code>tts --model_name tts_models/en/vctk/vits --text "this should be the epitome" --speaker_idx "p341"</code></p><p><strong>copy to good location</strong></p><p><code>!cp -r "/root/.local/share/tts" "~/tts"</code></p><p><strong>setup Tensorboard for the cool visuals and important stats</strong>
<img src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/images/tensorboard.png alt=[tensorboard]></p><p><strong>Set it as the the restore path</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python train.py --restore_path <span style=color:#e6db74>&#34;~/tts/vctk_model.pth&#34;</span> --config_path <span style=color:#e6db74>&#34;path_to_your_config_file&#34;</span>
</span></span></code></pre></div><p>yes , now we train,</p><p><img src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/images/hard.jpeg alt=[walter]></p><p><em>LET IT COOK</em></p><p>At around 40k steps we have usable results, Here are outputs for each of the five voices
for the text</p><p>&ldquo;މީތި މިހާރު މިދަނީ ވަރަށް ވެސް ހާޑު ކޮށް ހެން މަށަށް ހީވެ&rdquo;</p><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/madness_0.wav type=audio/mpeg></audio><figcaption>voice sample one</figcaption></figure><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/madness_1.wav type=audio/mpeg></audio><figcaption>voice sample two</figcaption></figure><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/madness_2.wav type=audio/mpeg></audio><figcaption>voice sample three</figcaption></figure><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/madness_3.wav type=audio/mpeg></audio><figcaption>voice sample four</figcaption></figure><figure><audio controls preload=metadata><source src=https://raw.githubusercontent.com/Dharisd/dharisd.github.io/main/assets/audio/madness_3.wav type=audio/mpeg></audio><figcaption>voice sample five</figcaption></figure><h2 id=how-do-i-generate-my-own-audio>How do i generate my own audio?</h2><p>First install the TTS library
<code>pip install tts</code></p><p>Download the models from <a href="https://drive.google.com/drive/folders/1OGVBHlttIjkRyKyxRklv-0CE8vACO3H3?usp=sharing">here</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tts --text <span style=color:#e6db74>&#34;މިހާރު ވާހަކަ ދެއްކޭނެ ވަރަށް ރަނގަޅު ވަރަކަށް&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --model_path <span style=color:#e6db74>&#34;checkpoint_1020000.pth&#34;</span><span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --config_path <span style=color:#e6db74>&#34;config.json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --speaker_idx <span style=color:#e6db74>&#34;MCV_f5a79d21b4ee20e7008f784477fae663361e7cb5d1f06ea88a7d246502364c6c1b1d273db9334ba1feb1ee712e349c9a50425c52345c95a3d2d2b5c9d59ee6b4&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --out_path output.wav
</span></span></code></pre></div><p>or you can use my synth <a href=https://colab.research.google.com/drive/1TMLBAcr-T9dSfItwvl6e_svl0FWi_JZn>notebook</a></p><h3 id=limitaions>Limitaions</h3><ul><li>Noticable background noise in some voices, this is manily due to ground truth also having noise</li><li>Pronounciation and tone are not as good as the TransformerTTS models from prior</li><li>The models are large at around 1 GB</li></ul><h3 id=how-to-improve>How to improve</h3><ul><li><p>Way better data needed
The models are only good as the data i give it, if the data is noisy the output is going to be noisy</p></li><li><p>Look into phonemiser
Could reduce complexity for the model and reduce the model size</p></li><li><p>More data
model did not improve much after 30k steps, could be due to lack of data</p></li></ul></section><div class=post-tags></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/dharisd rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/dharissss/ rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a></div><div class=footer-info>2023 © Dharis |</div></footer><script>feather.replace()</script></div></body></html>